{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token=\"\"\n",
    "PASSKEY = \"\"  # Change this to your secure passkey\n",
    "NGROK_TOKEN = \"\"  # Replace with your actual ngrok token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base packages\n",
    "!pip install -q transformers accelerate huggingface_hub torch flask flask_cors pyngrok\n",
    "!pip install -q langchain langchain-community sentence-transformers pypdf python-docx\n",
    "!pip install -q faiss-cpu ddgs beautifulsoup4 requests lxml pypdf2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, Response, stream_with_context\n",
    "from flask_cors import CORS\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
    "from huggingface_hub import login\n",
    "from pyngrok import ngrok\n",
    "import torch\n",
    "import threading\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Document processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "\n",
    "# Web search\n",
    "from ddgs import DDGS\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Loading embeddings model...\")\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "print(\"‚úÖ Embeddings model loaded!\")\n",
    "\n",
    "print(\"‚è≥ Loading Mistral model...\")\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"‚úÖ Mistral model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "vector_stores = {}\n",
    "episodic_memory = {}  # NEW: session_id -> list of {query, summary} objects\n",
    "USE_GPU = False\n",
    "gpu_resources = None\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            import faiss\n",
    "            test_res = faiss.StandardGpuResources()\n",
    "            test_index = faiss.IndexFlatL2(128)\n",
    "            gpu_test = faiss.index_cpu_to_gpu(test_res, 0, test_index)\n",
    "            USE_GPU = True\n",
    "            gpu_resources = test_res\n",
    "            print(f\"‚úÖ FAISS-GPU enabled on {torch.cuda.get_device_name(0)}\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è FAISS GPU not available, using CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è GPU check failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f9c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def llm_generate(prompt: str, max_tokens: int = 512, temperature: float = 0.1) -> str:\n",
    "    \"\"\"Generate text using Mistral model\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "def extract_text_from_txt(file_content):\n",
    "    try:\n",
    "        return file_content.decode('utf-8')\n",
    "    except:\n",
    "        return file_content.decode('latin-1')\n",
    "\n",
    "def extract_text_from_pdf(file_content):\n",
    "    pdf_file = io.BytesIO(file_content)\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(file_content):\n",
    "    doc_file = io.BytesIO(file_content)\n",
    "    doc = Document(doc_file)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def process_document(file_content, file_type):\n",
    "    if file_type == 'txt':\n",
    "        return extract_text_from_txt(file_content)\n",
    "    elif file_type == 'pdf':\n",
    "        return extract_text_from_pdf(file_content)\n",
    "    elif file_type in ['docx', 'doc']:\n",
    "        return extract_text_from_docx(file_content)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "\n",
    "def create_vector_store(text, session_id):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"üìÑ Created {len(chunks)} chunks\")\n",
    "\n",
    "    vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings_model)\n",
    "    vector_stores[session_id] = vectorstore\n",
    "    return len(chunks)\n",
    "\n",
    "def retrieve_relevant_chunks(query, session_id, k=3):\n",
    "    if session_id not in vector_stores:\n",
    "        return []\n",
    "    vectorstore = vector_stores[session_id]\n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    return [doc.page_content for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e65f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EPISODIC MEMORY SYSTEM (NEW!)\n",
    "# ============================================\n",
    "\n",
    "def init_episodic_memory(session_id: str):\n",
    "    \"\"\"Initialize episodic memory for session\"\"\"\n",
    "    global episodic_memory\n",
    "    if session_id not in episodic_memory:\n",
    "        episodic_memory[session_id] = []\n",
    "        print(f\"üß† Initialized episodic memory for {session_id}\")\n",
    "\n",
    "def add_to_episodic_memory(session_id: str, query: str, response: str):\n",
    "    \"\"\"Add query-response summary to episodic memory\"\"\"\n",
    "    global episodic_memory\n",
    "    init_episodic_memory(session_id)\n",
    "\n",
    "    # Generate 3-line summary of response\n",
    "    summary_prompt = f\"\"\"Summarize this response in exactly 3 short lines (max 50 words total).\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Response: {response[:500]}\n",
    "\n",
    "Write 3 concise lines:\"\"\"\n",
    "\n",
    "    try:\n",
    "        summary = llm_generate(summary_prompt, max_tokens=80, temperature=0.2)\n",
    "\n",
    "        # Clean up summary\n",
    "        lines = [line.strip() for line in summary.split('\\n') if line.strip()]\n",
    "        summary = ' '.join(lines[:3])  # Take first 3 lines\n",
    "\n",
    "        # Add to memory\n",
    "        memory_entry = {\n",
    "            \"query\": query,\n",
    "            \"summary\": summary\n",
    "        }\n",
    "\n",
    "        episodic_memory[session_id].append(memory_entry)\n",
    "\n",
    "        # Keep only last 15 entries to prevent memory overflow\n",
    "        if len(episodic_memory[session_id]) > 15:\n",
    "            episodic_memory[session_id] = episodic_memory[session_id][-15:]\n",
    "\n",
    "        print(f\"üíæ Added to episodic memory: '{query[:30]}...' ‚Üí '{summary[:50]}...'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to add episodic memory: {str(e)}\")\n",
    "\n",
    "def get_episodic_memory(session_id: str) -> str:\n",
    "    \"\"\"Get formatted episodic memory for prompt\"\"\"\n",
    "    global episodic_memory\n",
    "    init_episodic_memory(session_id)\n",
    "\n",
    "    if not episodic_memory[session_id]:\n",
    "        return \"No previous conversation history.\"\n",
    "\n",
    "    # Format memory as readable text\n",
    "    memory_text = \"Previous Conversation Summary:\\n\"\n",
    "    for i, entry in enumerate(episodic_memory[session_id], 1):\n",
    "        memory_text += f\"{i}. Q: {entry['query']}\\n   A: {entry['summary']}\\n\\n\"\n",
    "\n",
    "    return memory_text.strip()\n",
    "\n",
    "def get_last_query(session_id: str) -> str:\n",
    "    \"\"\"Get last query from episodic memory for context-aware rewriting\"\"\"\n",
    "    global episodic_memory\n",
    "    init_episodic_memory(session_id)\n",
    "\n",
    "    if episodic_memory[session_id]:\n",
    "        return episodic_memory[session_id][-1]['query']\n",
    "    return \"\"\n",
    "\n",
    "def clear_episodic_memory(session_id: str):\n",
    "    \"\"\"Clear episodic memory for session\"\"\"\n",
    "    global episodic_memory\n",
    "    if session_id in episodic_memory:\n",
    "        episodic_memory[session_id] = []\n",
    "        print(f\"üóëÔ∏è Cleared episodic memory for {session_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ca3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# WEB SEARCH FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def search_web(query, num_results=5):\n",
    "    try:\n",
    "        print(f\"üîç Searching DuckDuckGo for: {query}\")\n",
    "        urls = []\n",
    "        with DDGS() as ddgs:\n",
    "            for r in ddgs.text(query, max_results=num_results):\n",
    "                url = r.get('href') or r.get('url')\n",
    "                if url and url.startswith('http'):\n",
    "                    urls.append(url)\n",
    "                if len(urls) >= num_results:\n",
    "                    break\n",
    "        print(f\"‚úÖ Found {len(urls)} URLs\")\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå DuckDuckGo search error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def scrape_website(url, timeout=10):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe']):\n",
    "            element.decompose()\n",
    "\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "        text = '\\n'.join(lines)\n",
    "\n",
    "        if len(text) > 8000:\n",
    "            text = text[:8000]\n",
    "\n",
    "        print(f\"‚úÖ Scraped {len(text)} characters from {urlparse(url).netloc}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error scraping {url[:40]}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def search_and_scrape(query, num_sites=3):\n",
    "    print(f\"\\n{'='*60}\\nüåê Web Search: {query}\\n{'='*60}\\n\")\n",
    "    urls = search_web(query, num_results=num_sites * 2)\n",
    "\n",
    "    if not urls:\n",
    "        return None, \"No search results found.\"\n",
    "\n",
    "    combined_text = f\"# Web Results: {query}\\n\\n\"\n",
    "    successful_scrapes = 0\n",
    "\n",
    "    for url in urls[:num_sites * 2]:\n",
    "        if successful_scrapes >= num_sites:\n",
    "            break\n",
    "\n",
    "        text = scrape_website(url, timeout=15)\n",
    "        if text and len(text) > 200:\n",
    "            domain = urlparse(url).netloc\n",
    "            combined_text += f\"\\nSource {successful_scrapes + 1}: {domain}\\n{text}\\n\\n\"\n",
    "            successful_scrapes += 1\n",
    "\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    if successful_scrapes == 0:\n",
    "        return None, \"Failed to scrape websites.\"\n",
    "\n",
    "    print(f\"‚úÖ Scraped {successful_scrapes} sites\")\n",
    "    return combined_text, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694507de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ADVANCED MODE: CAG + SELF-RAG + AGENTIC RAG + EPISODIC MEMORY\n",
    "# ============================================\n",
    "\n",
    "def classify_intent(episodic_mem: str, query: str) -> str:\n",
    "    \"\"\"Classify user query intent using episodic memory\"\"\"\n",
    "\n",
    "    # Check if query is casual/simple\n",
    "    casual_patterns = [\n",
    "        'how are you', 'hey', 'hello', 'hi ', 'thanks', 'thank you',\n",
    "        'good morning', 'good night', 'bye', 'okay', 'ok', 'nice',\n",
    "        'great', 'cool', 'awesome', 'fine', 'alright'\n",
    "    ]\n",
    "\n",
    "    query_lower = query.lower().strip()\n",
    "    if any(pattern in query_lower for pattern in casual_patterns) or len(query.split()) <= 3:\n",
    "        print(f\"üéØ Intent: casual (no retrieval needed)\")\n",
    "        return \"casual\"\n",
    "\n",
    "    prompt = f\"\"\"Analyze the query and classify intent.\n",
    "\n",
    "{episodic_mem if episodic_mem != \"No previous conversation history.\" else \"No previous conversation.\"}\n",
    "\n",
    "Current query: \"{query}\"\n",
    "\n",
    "Classify the query as ONE word:\n",
    "- casual: Greetings, thanks, simple yes/no\n",
    "- new_question: Completely new topic needing information\n",
    "- follow_up: Continues current topic\n",
    "- clarification: Asks to explain previous answer differently\n",
    "\n",
    "Classification:\"\"\"\n",
    "\n",
    "    intent = llm_generate(prompt, max_tokens=5, temperature=0.05).strip().lower()\n",
    "\n",
    "    valid_intents = [\"casual\", \"follow_up\", \"new_question\", \"clarification\"]\n",
    "    for valid in valid_intents:\n",
    "        if valid in intent:\n",
    "            print(f\"üéØ Intent: {valid}\")\n",
    "            return valid\n",
    "\n",
    "    print(f\"üéØ Default: new_question\")\n",
    "    return \"new_question\"\n",
    "\n",
    "def rewrite_query_with_context(query: str, last_query: str) -> str:\n",
    "    \"\"\"Improved Self-RAG: Rewrite vague queries using last query context\"\"\"\n",
    "\n",
    "    # Check if query is vague (contains pronouns, \"it\", \"that\", etc.)\n",
    "    vague_indicators = ['it', 'this', 'that', 'these', 'those', 'what', 'how', 'why', 'where']\n",
    "    is_vague = any(word in query.lower().split()[:3] for word in vague_indicators) and len(query.split()) < 6\n",
    "\n",
    "    if not is_vague or not last_query:\n",
    "        # Query is clear, just add keywords\n",
    "        prompt = f\"\"\"Add 1-2 relevant keywords to this search query. Keep it concise.\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Enhanced query:\"\"\"\n",
    "        rewritten = llm_generate(prompt, max_tokens=20, temperature=0.1).strip()\n",
    "    else:\n",
    "        # Query is vague, use context from last query\n",
    "        prompt = f\"\"\"Rewrite this vague query using context from the previous question.\n",
    "\n",
    "Previous query: \"{last_query}\"\n",
    "\n",
    "Current vague query: \"{query}\"\n",
    "\n",
    "Rewrite as a clear, specific search query combining both contexts:\"\"\"\n",
    "        rewritten = llm_generate(prompt, max_tokens=30, temperature=0.1).strip()\n",
    "\n",
    "    # Clean and validate\n",
    "    rewritten = rewritten.replace('\"', '').replace(\"'\", \"\").strip()\n",
    "\n",
    "    # If rewrite fails or is too different, use original\n",
    "    if len(rewritten) < 3 or len(rewritten) > len(query) * 4:\n",
    "        print(f\"üìù Using original: '{query}'\")\n",
    "        return query\n",
    "\n",
    "    print(f\"üìù Rewritten: '{query}' ‚Üí '{rewritten}'\")\n",
    "    return rewritten\n",
    "\n",
    "def decide_retrieval_strategy(intent: str) -> Dict:\n",
    "    \"\"\"Agentic RAG: Decide retrieval strategy - smarter about when to retrieve\"\"\"\n",
    "    if intent == \"casual\":\n",
    "        return {\n",
    "            \"action\": \"no_retrieve\",\n",
    "            \"doc_k\": 0,\n",
    "            \"web_k\": 0,\n",
    "            \"use_web\": False,\n",
    "            \"reason\": \"Casual chat - no retrieval needed\"\n",
    "        }\n",
    "    elif intent == \"new_question\":\n",
    "        return {\n",
    "            \"action\": \"retrieve\",\n",
    "            \"doc_k\": 3,\n",
    "            \"web_k\": 3,\n",
    "            \"use_web\": True,\n",
    "            \"reason\": \"New topic - full retrieval\"\n",
    "        }\n",
    "    elif intent == \"follow_up\":\n",
    "        return {\n",
    "            \"action\": \"retrieve_constrained\",\n",
    "            \"doc_k\": 2,\n",
    "            \"web_k\": 2,\n",
    "            \"use_web\": True,\n",
    "            \"reason\": \"Follow-up - focused retrieval\"\n",
    "        }\n",
    "    elif intent == \"clarification\":\n",
    "        return {\n",
    "            \"action\": \"no_retrieve\",\n",
    "            \"doc_k\": 0,\n",
    "            \"web_k\": 0,\n",
    "            \"use_web\": False,\n",
    "            \"reason\": \"Clarification - memory only\"\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"action\": \"retrieve\",\n",
    "            \"doc_k\": 3,\n",
    "            \"web_k\": 3,\n",
    "            \"use_web\": True,\n",
    "            \"reason\": \"Default strategy\"\n",
    "        }\n",
    "\n",
    "def retrieve_with_advanced_strategy(\n",
    "    query: str,\n",
    "    session_id: str,\n",
    "    strategy: Dict\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Advanced retrieval: doc + web chunks (no raw history)\"\"\"\n",
    "\n",
    "    doc_chunks = []\n",
    "    web_chunks = []\n",
    "\n",
    "    if strategy[\"action\"] == \"no_retrieve\":\n",
    "        print(\"üö´ Skipping retrieval (clarification mode)\")\n",
    "        return doc_chunks, web_chunks\n",
    "\n",
    "    # Document Retrieval\n",
    "    if session_id in vector_stores and strategy[\"doc_k\"] > 0:\n",
    "        doc_chunks = retrieve_relevant_chunks(query, session_id, k=strategy[\"doc_k\"])\n",
    "        if doc_chunks:\n",
    "            print(f\"üìö Retrieved {len(doc_chunks)} document chunks\")\n",
    "\n",
    "    # Web Retrieval\n",
    "    if strategy[\"use_web\"] and strategy[\"web_k\"] > 0:\n",
    "        print(\"üåê Performing web search...\")\n",
    "        web_text, error = search_and_scrape(query, num_sites=2)\n",
    "\n",
    "        if web_text and not error:\n",
    "            web_session_id = f\"web_adv_{session_id}_{abs(hash(query))}\"\n",
    "            create_vector_store(web_text, web_session_id)\n",
    "            web_chunks = retrieve_relevant_chunks(query, web_session_id, k=strategy[\"web_k\"])\n",
    "            if web_chunks:\n",
    "                print(f\"üåê Retrieved {len(web_chunks)} web chunks\")\n",
    "\n",
    "    return doc_chunks, web_chunks\n",
    "\n",
    "def build_prompt_with_episodic_memory(\n",
    "    episodic_mem: str,\n",
    "    query: str,\n",
    "    doc_chunks: List[str],\n",
    "    web_chunks: List[str],\n",
    "    intent: str\n",
    ") -> str:\n",
    "    \"\"\"Build prompt using episodic memory - smart about using retrieved data\"\"\"\n",
    "\n",
    "    # For casual queries, only use memory\n",
    "    if intent == \"casual\":\n",
    "        return f\"\"\"You are a friendly AI assistant. Respond naturally to casual conversation.\n",
    "\n",
    "{episodic_mem if episodic_mem != \"No previous conversation history.\" else \"\"}\n",
    "\n",
    "User: {query}\n",
    "\n",
    "Respond in a natural, conversational way (1-2 sentences):\"\"\"\n",
    "\n",
    "    # For clarification, use memory only\n",
    "    if intent == \"clarification\":\n",
    "        # Extract last 2 interactions for context\n",
    "        if episodic_mem != \"No previous conversation history.\":\n",
    "            lines = episodic_mem.split('\\n')\n",
    "            relevant = [l for l in lines if l.strip() and not l.startswith('Previous')]\n",
    "            if len(relevant) > 4:\n",
    "                recent_context = '\\n'.join(relevant[-4:])\n",
    "            else:\n",
    "                recent_context = episodic_mem\n",
    "        else:\n",
    "            recent_context = episodic_mem\n",
    "\n",
    "        return f\"\"\"Clarify or rephrase the previous explanation.\n",
    "\n",
    "{recent_context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Provide a clearer explanation:\"\"\"\n",
    "\n",
    "    # For questions needing information, intelligently use available sources\n",
    "    sources_available = []\n",
    "    if doc_chunks:\n",
    "        sources_available.append(\"document\")\n",
    "    if web_chunks:\n",
    "        sources_available.append(\"web\")\n",
    "\n",
    "    # Build context sections only if data is available\n",
    "    context_parts = []\n",
    "\n",
    "    if doc_chunks:\n",
    "        doc_text = \"\\n\".join([f\"- {chunk[:250]}\" for chunk in doc_chunks])\n",
    "        context_parts.append(f\"Document Information:\\n{doc_text}\")\n",
    "\n",
    "    if web_chunks:\n",
    "        web_text = \"\\n\".join([f\"- {chunk[:250]}\" for chunk in web_chunks])\n",
    "        context_parts.append(f\"Web Information:\\n{web_text}\")\n",
    "\n",
    "    # Add memory for continuity\n",
    "    memory_section = \"\"\n",
    "    if episodic_mem != \"No previous conversation history.\":\n",
    "        # Use only last 2 interactions for short-term context\n",
    "        lines = episodic_mem.split('\\n')\n",
    "        relevant = [l for l in lines if l.strip() and not l.startswith('Previous')]\n",
    "        if len(relevant) > 4:\n",
    "            memory_section = \"Recent Context:\\n\" + '\\n'.join(relevant[-4:])\n",
    "\n",
    "    # Build final prompt\n",
    "    prompt_parts = [\"Answer the question accurately and concisely.\"]\n",
    "\n",
    "    if memory_section:\n",
    "        prompt_parts.append(memory_section)\n",
    "\n",
    "    if context_parts:\n",
    "        prompt_parts.append(\"\\n\".join(context_parts))\n",
    "        prompt_parts.append(f\"\\nQuestion: {query}\")\n",
    "        prompt_parts.append(\"\\nInstructions:\\n- Use only the relevant information provided\\n- Don't mix unrelated sources\\n- Be direct and concise\\n- If info insufficient, state clearly\")\n",
    "    else:\n",
    "        prompt_parts.append(f\"\\nQuestion: {query}\")\n",
    "        prompt_parts.append(\"\\nAnswer based on your knowledge:\")\n",
    "\n",
    "    prompt_parts.append(\"\\nAnswer:\")\n",
    "\n",
    "    return \"\\n\\n\".join(prompt_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STREAMING GENERATION\n",
    "# ============================================\n",
    "\n",
    "def generate_stream(prompt: str):\n",
    "    \"\"\"Streaming generation\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    def run_generation():\n",
    "        with torch.inference_mode():\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=600,\n",
    "                use_cache=True,\n",
    "                streamer=streamer,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "    thread = threading.Thread(target=run_generation)\n",
    "    thread.start()\n",
    "\n",
    "    for token in streamer:\n",
    "        yield token\n",
    "\n",
    "def episodic_memory_streamer(generator, session_id: str, user_query: str):\n",
    "    \"\"\"Stream and update episodic memory after completion\"\"\"\n",
    "    buffer = \"\"\n",
    "    for token in generator:\n",
    "        buffer += token\n",
    "        yield token\n",
    "\n",
    "    # Add to episodic memory after response complete\n",
    "    if buffer.strip():\n",
    "        add_to_episodic_memory(session_id, user_query, buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4dfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FLASK APP\n",
    "# ============================================\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/upload-document\", methods=[\"POST\", \"OPTIONS\"])\n",
    "def upload_document():\n",
    "    if request.method == \"OPTIONS\":\n",
    "        return jsonify({\"status\": \"ok\"}), 200\n",
    "\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "\n",
    "        if data.get(\"passkey\") != PASSKEY:\n",
    "            return jsonify({\"error\": \"Unauthorized\"}), 403\n",
    "\n",
    "        file_base64 = data.get(\"file_content\")\n",
    "        file_name = data.get(\"file_name\")\n",
    "        session_id = data.get(\"session_id\", \"default\")\n",
    "\n",
    "        if not file_base64 or not file_name:\n",
    "            return jsonify({\"error\": \"File content and name required\"}), 400\n",
    "\n",
    "        file_content = base64.b64decode(file_base64)\n",
    "        file_type = file_name.split('.')[-1].lower()\n",
    "\n",
    "        print(f\"üì• Processing {file_name}\")\n",
    "        text = process_document(file_content, file_type)\n",
    "\n",
    "        if not text.strip():\n",
    "            return jsonify({\"error\": \"No text extracted\"}), 400\n",
    "\n",
    "        print(f\"üìù Extracted {len(text)} characters\")\n",
    "        num_chunks = create_vector_store(text, session_id)\n",
    "\n",
    "        return jsonify({\n",
    "            \"message\": \"Document processed\",\n",
    "            \"num_chunks\": num_chunks,\n",
    "            \"session_id\": session_id\n",
    "        }), 200\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route(\"/query\", methods=[\"POST\", \"OPTIONS\"])\n",
    "def query_model():\n",
    "    if request.method == \"OPTIONS\":\n",
    "        return jsonify({\"status\": \"ok\"}), 200\n",
    "\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "\n",
    "        if data.get(\"passkey\") != PASSKEY:\n",
    "            return jsonify({\"error\": \"Unauthorized\"}), 403\n",
    "\n",
    "        query = data.get(\"query\", \"\")\n",
    "        mode = data.get(\"mode\", \"model\")\n",
    "        session_id = data.get(\"session_id\", \"default\")\n",
    "\n",
    "        if not query:\n",
    "            return jsonify({\"error\": \"Query missing\"}), 400\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîç Query: {query}\")\n",
    "        print(f\"üìã Mode: {mode}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        # Initialize episodic memory\n",
    "        init_episodic_memory(session_id)\n",
    "\n",
    "        # Get episodic memory\n",
    "        episodic_mem = get_episodic_memory(session_id)\n",
    "\n",
    "        # ADVANCED MODE\n",
    "        if mode == \"advanced\":\n",
    "            print(\"üöÄ ADVANCED MODE (Episodic Memory)\")\n",
    "\n",
    "            # Classify intent\n",
    "            intent = classify_intent(episodic_mem, query)\n",
    "\n",
    "            # Get last query for context\n",
    "            last_query = get_last_query(session_id)\n",
    "\n",
    "            # Rewrite query with context\n",
    "            search_query = query\n",
    "            if intent == \"new_question\" or intent == \"follow_up\":\n",
    "                search_query = rewrite_query_with_context(query, last_query)\n",
    "\n",
    "            # Get retrieval strategy\n",
    "            strategy = decide_retrieval_strategy(intent)\n",
    "            print(f\"üéØ Strategy: {strategy['reason']}\")\n",
    "\n",
    "            # Retrieve\n",
    "            doc_chunks, web_chunks = retrieve_with_advanced_strategy(\n",
    "                search_query,\n",
    "                session_id,\n",
    "                strategy\n",
    "            )\n",
    "\n",
    "            # Build prompt\n",
    "            prompt = build_prompt_with_episodic_memory(\n",
    "                episodic_mem,\n",
    "                query,\n",
    "                doc_chunks,\n",
    "                web_chunks,\n",
    "                intent\n",
    "            )\n",
    "\n",
    "            # Stream response\n",
    "            return Response(\n",
    "                stream_with_context(\n",
    "                    episodic_memory_streamer(\n",
    "                        generate_stream(prompt),\n",
    "                        session_id,\n",
    "                        query\n",
    "                    )\n",
    "                ),\n",
    "                mimetype='text/event-stream',\n",
    "                headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'}\n",
    "            )\n",
    "\n",
    "        # DOCUMENT MODE\n",
    "        elif mode == \"document\":\n",
    "            context_chunks = retrieve_relevant_chunks(query, session_id, k=3)\n",
    "\n",
    "            if not context_chunks:\n",
    "                return jsonify({\"error\": \"No document uploaded\"}), 400\n",
    "\n",
    "            context = \"\\n\\n\".join(context_chunks)\n",
    "            print(f\"üìö Retrieved {len(context_chunks)} chunks\")\n",
    "\n",
    "            # Build prompt with episodic memory\n",
    "            prompt = f\"\"\"Answer using the provided information.\n",
    "\n",
    "{episodic_mem}\n",
    "\n",
    "Document Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "            return Response(\n",
    "                stream_with_context(\n",
    "                    episodic_memory_streamer(generate_stream(prompt), session_id, query)\n",
    "                ),\n",
    "                mimetype='text/event-stream',\n",
    "                headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'}\n",
    "            )\n",
    "\n",
    "        # WEB MODE\n",
    "        elif mode == \"web\":\n",
    "            # Get last query for context\n",
    "            last_query = get_last_query(session_id)\n",
    "\n",
    "            # Rewrite query with context\n",
    "            search_query = rewrite_query_with_context(query, last_query)\n",
    "\n",
    "            web_text, error = search_and_scrape(search_query, num_sites=3)\n",
    "\n",
    "            if error:\n",
    "                return jsonify({\"error\": error}), 400\n",
    "\n",
    "            web_session_id = f\"web_{session_id}_{abs(hash(query))}\"\n",
    "            num_chunks = create_vector_store(web_text, web_session_id)\n",
    "            print(f\"üìÑ Created {num_chunks} chunks\")\n",
    "\n",
    "            context_chunks = retrieve_relevant_chunks(query, web_session_id, k=4)\n",
    "            context = \"\\n\\n\".join(context_chunks)\n",
    "\n",
    "            # Build prompt with episodic memory\n",
    "            prompt = f\"\"\"Answer using web information.\n",
    "\n",
    "{episodic_mem}\n",
    "\n",
    "Web Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "            return Response(\n",
    "                stream_with_context(\n",
    "                    episodic_memory_streamer(generate_stream(prompt), session_id, query)\n",
    "                ),\n",
    "                mimetype='text/event-stream',\n",
    "                headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'}\n",
    "            )\n",
    "\n",
    "        # MODEL MODE\n",
    "        else:\n",
    "            # Check if it's a casual query\n",
    "            casual_patterns = ['how are you', 'hey', 'hello', 'hi ', 'thanks', 'thank you', 'bye', 'okay', 'ok']\n",
    "            is_casual = any(pattern in query.lower() for pattern in casual_patterns) or len(query.split()) <= 3\n",
    "\n",
    "            if is_casual:\n",
    "                prompt = f\"\"\"Respond naturally to this casual message.\n",
    "\n",
    "{episodic_mem if episodic_mem != \"No previous conversation history.\" else \"\"}\n",
    "\n",
    "User: {query}\n",
    "\n",
    "Respond in 1-2 sentences:\"\"\"\n",
    "            else:\n",
    "                prompt = f\"\"\"{episodic_mem}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer naturally:\"\"\"\n",
    "\n",
    "            return Response(\n",
    "                stream_with_context(\n",
    "                    episodic_memory_streamer(generate_stream(prompt), session_id, query)\n",
    "                ),\n",
    "                mimetype='text/event-stream',\n",
    "                headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'}\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route(\"/clear-document\", methods=[\"POST\", \"OPTIONS\"])\n",
    "def clear_document():\n",
    "    if request.method == \"OPTIONS\":\n",
    "        return jsonify({\"status\": \"ok\"}), 200\n",
    "\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        session_id = data.get(\"session_id\", \"default\")\n",
    "\n",
    "        if session_id in vector_stores:\n",
    "            del vector_stores[session_id]\n",
    "\n",
    "        clear_episodic_memory(session_id)\n",
    "\n",
    "        return jsonify({\"message\": \"Document and memory cleared\"}), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route(\"/clear-web-cache\", methods=[\"POST\", \"OPTIONS\"])\n",
    "def clear_web_cache():\n",
    "    if request.method == \"OPTIONS\":\n",
    "        return jsonify({\"status\": \"ok\"}), 200\n",
    "\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        session_id = data.get(\"session_id\", \"default\")\n",
    "\n",
    "        web_sessions = [k for k in vector_stores.keys() if k.startswith(f\"web_{session_id}\") or k.startswith(f\"web_adv_{session_id}\")]\n",
    "\n",
    "        for ws in web_sessions:\n",
    "            del vector_stores[ws]\n",
    "\n",
    "        return jsonify({\"message\": f\"Cleared {len(web_sessions)} entries\"}), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route(\"/health\", methods=[\"GET\"])\n",
    "def health():\n",
    "    doc_sessions = len([k for k in vector_stores.keys() if not k.startswith(\"web_\")])\n",
    "    web_sessions = len([k for k in vector_stores.keys() if k.startswith(\"web_\")])\n",
    "\n",
    "    return jsonify({\n",
    "        \"status\": \"ok\",\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"active_sessions\": {\n",
    "            \"documents\": doc_sessions,\n",
    "            \"web_cache\": web_sessions,\n",
    "            \"episodic_memory\": len(episodic_memory)\n",
    "        },\n",
    "        \"gpu\": torch.cuda.is_available(),\n",
    "        \"features\": [\n",
    "            \"episodic_memory_system\",\n",
    "            \"context_aware_rewriting\",\n",
    "            \"hybrid_memory\",\n",
    "            \"all_modes_memory_enabled\"\n",
    "        ]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start server\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "public_url = ngrok.connect(5000).public_url\n",
    "print(f\"üîó API Endpoint: {public_url}\")\n",
    "print(f\"üîë Passkey: {PASSKEY}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f73118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c347de48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fbab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55230167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da06826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe06d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acabea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88745d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
